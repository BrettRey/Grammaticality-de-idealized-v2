# Reviewers
I don’t know what *Chanel David Lewis* in particular would actually write (and I can’t find a stable academic profile/publications for that exact name via a quick public search), so what follows is a simulated reviewer report in the register of a tough-but-fair reviewer who is (i) methodologically picky about what counts as evidence, (ii) allergic to competence/performance handwaving, and (iii) attentive to normativity, ideology, and power in “repertoire” talk. Use it as an adversarial stress test.

---

REVIEWER REPORT (simulated)

Manuscript: “The asterisk de-idealized: Looking back at grammaticality, moving forward with conditioned stability”

Summary and overall assessment
The paper makes a serious bid to reframe what the asterisk has been doing in theoretical linguistics. The central move is to treat grammaticality as a state of conditioned stability of form–value relations in a construed communicative situation, decomposed into (i) mapping viability (map), (ii) interpretive coherence (K), and (iii) repertoire status (C_t), with a multiplicative stability score and a situation-sensitive threshold τ(c) determining categorical membership. The paper’s strongest features are (a) the insistence that “grammaticality” has been asked to answer multiple distinct questions at once, (b) the explicit separation of grammatical status from the feeling of ungrammaticality, and (c) the attempt to build falsifiability and measurement commitments into the proposal rather than leaving them as promissory notes.

I think the project is publishable in principle, and the “Looking Back, Moving Forward” frame is apt. At present, however, several key notions are under-specified in ways that risk making the framework too permissive (or, conversely, inadvertently circular). In particular: the empirical meaning of map = 0 is unclear; the K vs C_t separation is promising but currently rests too heavily on intuitive glosses; and the treatment of “norm-centres” and “repertoire policing” needs a more explicit engagement with standard-language ideology and the social asymmetries that determine whose repertoires count. The formalism is not the problem; the evidential hooks and sociopolitical ontology are.

Recommendation: Major revision (revise and resubmit)

Major comments

1. The map component: structural “crash” needs sharper empirical and theoretical criteria
   You want map(u,c) ∈ {0,1} to isolate genuine analyzability failure. But the paper’s showcase crash example (⟨Can the have running?⟩) is not obviously a crash in any strong sense: many readers will be able to assign categories, build some constituent structure, and even coerce a kind of metalinguistic or jocular parse. More generally, “no viable morphosyntactic analysis” is hard to make non-circular unless you specify which representational commitments and typing conditions are in play.

What I think you need here is one of the following (preferably both):

* An explicit definition of “well-typed representation” that is not just “licensed by the grammar”, otherwise map collapses into the very notion you are trying to deconflate. If map is meant to be pre-semantic parsing viability, say what counts as a viable parse in a cognitively realistic parsing system (and what kinds of repairs/reanalyses are permitted).
* A better set of “map = 0” diagnostics than “it feels like nonsense”. You gesture at parse failure, repair failure, comprehension collapse under supportive contexts. Good. But you need at least one worked case where you show that supportive contexts, prosody, and pragmatic reconstruction do not rescue mapping.

As written, the paper sometimes treats “structural crash” as common and “diagnostically clean”. I suspect the opposite: true crash is rare, and many “nonsense” strings fail because of selection/value conflicts and discourse-level unavailability rather than because no parse exists. If you keep map binary, you must defend why gradience/partial parses belong elsewhere (processing, K, etc.) rather than in map.

2. K vs C_t: the separation is the lynchpin; right now it is more asserted than demonstrated
   The framework lives or dies by whether coherence failure (low K) and repertoire exclusion (low C_t) are empirically separable in a way that does not depend on asking participants to introspect the distinction for you (e.g., “I know what you mean, but we don’t say that”).

You propose paraphrase dispersion and inference-task variability for K, and production/opportunity-normalised frequency for C_t. That is the right direction, but you should tighten it into something closer to an explicit measurement model:

* For K: specify at least one concrete operationalisation (e.g., forced-choice among competing paraphrases plus confidence; entropy of paraphrase distribution; response-time cost to resolve temporal anchoring; consistency in truth-value judgments across minimal scenario manipulations).
* For C_t: specify at least one concrete operationalisation that is not reducible to acceptability ratings. You mention “would you say this?” and production probability, plus opportunity-normalised corpus rates and repair behaviour. Good. But show how these indicators discriminate (i) “rare but potentially in repertoire given small niche mass” vs (ii) “systematically excluded despite large niche mass”.

Right now the paper leans on ⟨I have 25 years⟩ as paradigmatic low C_t with high K. But one could argue (depending on population and conditioning) that the main degradation is ideological: hearers classify it as “learner English” and therefore treat it as illegitimate even when they understand it. That is exactly the point you want to make—except then you must say how ideology relates to C_t rather than leaving it as an external “filtering” term that only contaminates ratings. If ideology affects whether something counts as a legitimate resource in a norm-centre, it is constitutive of C_t, not merely a ratings contaminant.

3. Conditioning state c: you need a clearer ontology of “norm-centres” and a power-aware account of whose norms do the conditioning
   The S/A/I decomposition is one of the manuscript’s most useful proposals, because it pushes “context” from a handwave to an experimental design space. However, the current treatment risks reifying “norm-centres” as stable, consensual objects, when much of the sociolinguistic and linguistic-anthropological literature emphasises that:

* normativity is contested and stratified;
* what counts as “the relevant” norm-centre is often imposed institutionally;
* ascription (A) is not just an inference variable but a site of racialising and classing power.

You do acknowledge ideological filtering, gatekeeping, and prescriptive correctness. But the paper would be stronger if you explicitly align your “norm-centre” apparatus with standard-language ideology and raciolinguistic work (without turning the paper into a different paper). Two concrete revisions:

* Make explicit that c is not merely “construed” but often *policed* under asymmetric consequences, and that τ(c) can be institutionally enforced in ways that reshape C_t trajectories over time.
* Clarify whether C_t is meant to track “what the community treats as legitimate” in a descriptive sense that already includes power (my reading), or whether you are trying to hold power constant and treat it as exogenous. The current architecture splits “ideological filtering” off as a ratings modifier; that division is not obviously coherent if ideology is part of how repertoires are constituted and maintained.

4. The product rule: the interaction prediction is good; the independence assumptions need guarding
   The argument for a non-compensatory operator is reasonable, and you are right that min vs product is empirically discriminable given factorial manipulations. But you should address two threats:

* Dependence: K and C_t will often be correlated because unfamiliar/out-of-repertoire forms can induce interpretive dispersion (people disagree about what the form is “supposed” to mean). You note this worry but could do more to constrain it. One option is to model K as “coherence given stipulated conventional values”, while C_t is “probability the stipulation is community-legitimate”. That makes the separation sharper.
* Thresholding: τ(c) risks being used as an escape hatch even with your “not construction-specific” constraint, because in practice researchers can smuggle construction specificity into the choice of anchors for calibration. You could pre-empt this by specifying what an anchor set must include (e.g., clear in-repertoire operator contrasts, clear out-of-repertoire variants, plus one or two known processing illusions) and how anchors are selected independently of the target phenomenon.

5. Opportunity normalisation: promising, but one worked example is not enough to secure the methodological claim
   The “worked opportunity proxy” subsection is a highlight; it is also where I would expect a sceptical reader to push hardest. Using competitor tokens as a lower-bound for opportunities is sensible, but it comes with pitfalls you should acknowledge and, where possible, address:

* competitors are not always substitutable without changing stance/register/social meaning;
* corpora slice c imperfectly; conditioning can vary within a corpus genre;
* “absence despite large N*” can reflect annotation/search limitations.

You don’t need to solve these problems, but you should state conditions under which the proxy is invalid and how your disconfirmation criteria would detect that invalidity. Otherwise the opportunity move risks looking like a dressed-up argument from absence.

6. Positioning in existing debates: you are close to several literatures you don’t quite name
   A reader will notice that the proposal resembles (and could be strengthened by) explicit links to:

* convention/game-theoretic accounts of linguistic coordination (your own later discussion gestures here; bringing the connection forward would make the “state” ontology feel less ad hoc);
* probabilistic/gradient grammar approaches that already separate grammatical knowledge from acceptability measurement;
* sociolinguistic work on enregisterment and legitimacy that treats “repertoire” as socially ratified.

I’m not asking for a literature review. I’m asking for a few strategically placed “this differs from X because…” sentences so that the contribution reads as an integration rather than a rebranding.

Minor comments and local revisions

* The opening cluster does a lot of work. Consider tightening the mapping between each example and the component diagnosis, especially for the alleged map=0 case(s). If you cannot find a robust map=0 example that survives supportive context, revise the narrative: make map=0 rarer but conceptually important.
* The label “structural crash” may invite pushback from readers who think in terms of repair and coercion. You can keep it, but define it behaviourally (what fails in comprehension/production) rather than impressionistically.
* The discussion of ⟨I’ve finished it yesterday⟩ vacillates (productively) between low K and low C_t diagnoses. That’s fine, but you should treat it explicitly as a case study showing how the framework adjudicates rather than as an example that “illustrates” one box in the typology.
* If you are going to cite forthcoming/in-prep work (including your own), make sure each such citation is either accessible (preprint) or replaceable with an already-public source. Some journals will push back hard here.
* The “large language models” paragraph is unusually detailed for acknowledgements. Depending on journal policy, you may want to move to a brief disclosure statement or shorten.

What I would look for in a revised version (concrete targets)

1. A tightened definition of map with at least one “hard” diagnostic case that is not merely semantically odd.
2. A short, explicit measurement sketch: one task battery that estimates K separately from C_t, with predicted dissociations and failure modes.
3. A power-aware clarification: is ideology constitutive of repertoire status (C_t), merely a contaminant of ratings, or both via different pathways? Make the architecture reflect your answer.
4. A more explicit constraint on τ(c) estimation (anchor selection and preregisterable calibration logic).
5. A second opportunity-normalised worked example (even a brief one) showing how the method discriminates “rare niche” from “stable gap”.

---

If you tell me what you mean by “Chanel David Lewis” (e.g., a specific scholar’s research area or a particular reviewer temperament you’re trying to anticipate), I can re-cut this into a more faithful “voice” (more sociophonetic, more semantic/pragmatic, more philosophy-of-language, more method-policing, more adversarial, etc.).

I can’t write a report *as if I were* a specific real person (Carson T. Schütze, or anyone else). What I can do is give you an anonymous-style referee report written from the standpoint of a reviewer whose methodological priors are close to what’s argued in work like *Schütze (2016)* on grammaticality judgements and evidence, and who is likely to be sympathetic but demanding about measurement models.

---

REFEREE REPORT (anonymous)

Summary and main contribution

The paper argues that the asterisk has been asked to do incompatible work: it is routinely used to mark (i) structural crash (no viable analysis), (ii) interpretive instability/incoherence under the live constraints of a communicative situation, (iii) repertoire exclusion (a form–value relation not licensed as a community resource under a norm-centre), and (iv) negative affect/“feeling of ungrammaticality” driven by processing or ideology. The proposed fix is a “minimal state theory” that reconceptualises grammaticality as *conditioned stability* of form–value relations relative to a conditioning state (c) (situation/ascription/identification), decomposed into three constitutive quantities: mapping viability (\mathsf{map}\in{0,1}), interpretive coherence (K\in[0,1]), and repertoire status (C_t\in[0,1]). These combine multiplicatively into a graded stability score (\widetilde{G}_t=\mathsf{map}\cdot K\cdot C_t), with categorical membership derived by thresholding against a situation-specific criterion (\tau(c)).

The paper’s strongest move is the explicit separation of (a) a claim about *ontological state* (what grammatical status consists in) from (b) a claim about *measurement channels* (why acceptability ratings are noisy and can dissociate from status). The figures and falsification section push in the right direction: the proposal aims to be empirically vulnerable rather than merely terminological.

Recommendation

Major revisions. The core decomposition is promising, but several components are currently underdetermined in ways that will invite the standard methodology objection: without a clearer measurement model and sharper operational commitments, the framework risks becoming a relabelling scheme that can fit many outcomes post hoc. The paper already anticipates this worry (especially in the discussion of (\tau(c)) and in §“What would count against this framework?”), but it needs to do more of the constraining work *inside the main argument* rather than in a final falsification checklist.

Major comments

1. The “asterisk conflation” diagnosis needs a tighter target

The opening cluster is persuasive as a pedagogical set, but the paper sometimes implies a stronger sociological claim than it actually establishes: that *the field* uses “*” as a single undifferentiated label for all these failure types. In practice, many authors already exploit a small typology (*, ?, #, etc.), and some subfields reserve “#” specifically for semantic/pragmatic anomaly. You do use # and ? yourself, which raises the question: what exactly is the target of “conflation”?

A way to sharpen this without changing the thesis:

* Treat the claim as: even where multiple diacritics exist, the *theoretical role* of “ungrammaticality” in argumentation still collapses distinct questions (viability vs coherence vs repertoire), because the inferential leap from a judgement mark to a claim about “the grammar” remains underspecified.
* Then show, with one or two concrete examples from the literature (briefly), how authors slide from (say) rating differences to claims about competence/representation, or from norm-centred exclusion to structural ban, without an explicit measurement model.

As written, the historical narrative is plausible, but a sceptical reviewer will ask for at least minimal evidential anchoring of the “conflation in practice” claim, beyond the (excellent) constructed cluster.

2. (\mathsf{map}) as binary: defend the idealisation, and delimit its scope

You stipulate (\mathsf{map}(u,c)\in{0,1}) as “whether there exists at least one viable morphosyntactic analysis … with a well-typed representation.” This is clean, but it will trigger predictable pushback:

* Many “crash” cases are crashy because of *analyst* commitments (what counts as an allowable repair, coercion, category shift, ellipsis, etc.), not because the string is intrinsically unanalyzable for speakers.
* Even within a fixed formalism, partial analyses, uncertain category assignments, and competing parses suggest a more graded or probabilistic notion of analyzability.

You can keep the binary idealisation, but it needs explicit boundaries:

* Say more concretely what counts as “mapping viability” evidence in humans (your Table 1 gestures to parse failure / no stable category assignment, but this is exactly where reviewers will demand operationalisation).
* Clarify how you treat “initial crash then repair succeeds” cases (you briefly do this with *The old man the boats*). As it stands, those examples imply that (\mathsf{map}=1) but the *process* fails; that’s fine, but then map-failure proper should be characterised as robust to supportive reanalysis and instruction. Spell that out as an operational criterion.

A crisp paragraph that distinguishes (i) *no humanly recoverable analysis even with guidance* vs (ii) *analysis exists but is hard to stabilise online* would pre-empt a lot of resistance.

3. The (K) / (C_t) separation is central; the paper needs a more committal measurement story

You propose a useful diagnostic contrast (Table 2): low-(K) items show paraphrase dispersion and meaning disagreement; low-(C_t) items show stable construal but “we don’t say that.” That’s exactly the kind of empirical wedge a sceptical methodology reviewer will insist on.

Two requests here:

(a) Commit to at least one concrete operationalisation for (K)

Right now (K) is introduced as “concentration of a distribution over candidate construals.” That’s attractive, but still schematic. Add one worked mini-protocol, at the level of detail of your age-stating opportunity proxy:

* What’s the task? (free paraphrase, forced-choice paraphrase selection, inference questions, etc.)
* What’s the quantification? (entropy over paraphrase-choice distribution; inter-annotator agreement on construal categories; variance of scalar inference responses; etc.)
* What pattern counts as “high dispersion” vs “stable”?

You don’t need a full methods paper, but without *one* explicit instantiation, (K) will read as a placeholder.

(b) Tighten what (C_t) is a probability *of*

“Population-level posterior probability that the form–value relation is in the community’s repertoire for (c)” is close, but “treats as a legitimate option” is exactly where correctness/ideology pressures can sneak in.

A reviewer will likely ask you to separate at least three notions that can come apart empirically:

* “I recognise this as belonging to Variety X” (enregisterment/recognition),
* “I would produce this under (c)” (production probability),
* “I endorse this as appropriate/correct under (c)” (evaluation/policing).

Your Fig. 7 (“ideological filtering”) acknowledges the last factor in ratings, but (C_t) itself currently includes “legitimate resource rather than error,” which can be read as partly evaluative. If you mean “licensed as a resource within the norm-centre being oriented to,” you can keep it realist without smuggling in prescriptivism—but you need to say how you will *measure* it without collapsing into endorsement.

A practical way to do this: treat (C_t) as anchored primarily in (i) production-choice tasks under controlled norm-centre framing plus (ii) opportunity-normalised corpus rates; treat overt endorsement/policing as a separate observable that correlates with—but is not identical to—(C_t).

4. Opportunity normalisation is a strong methodological move; it needs one more guardrail

The age-stating proxy in §\ref{sec:opportunity-worked} is a good illustration of why “zero tokens” can be evidential when competitor opportunity is large. But a reviewer will raise two concerns that should be headed off:

* Competitor tokens witness *some* opportunity, but not necessarily equal opportunity for all variants (register differences, syntactic priming, audience design, and topic distribution can skew the apparent opportunity set).
* Corpora are mixtures of conditioning states (c); your own theory makes this non-ignorable.

You already have the conceptual machinery to address this; the paper just needs to apply it:

* Explicitly state that opportunity proxies must be computed within a corpus slice that approximates (c) (you gesture to this).
* Give one concrete criterion for “approximates (c)” beyond “a conversational corpus” (genre + participant relations; or a corpus with metadata enabling norm-centre inference; or at least triangulation across multiple corpora).
* Flag the possibility of Simpson’s-paradox-like artefacts when aggregating across norm-centres, and treat that as a methodological motivation for conditioning rather than as an afterthought.

5. (\tau(c)) and stakes: good idea, but you need a clearer separation between *status* and *decision*

Your decision-theoretic story for (\tau(c)) is sensible, and you explicitly try to prevent “(\tau) can save anything” by making (\tau) state-wide rather than construction-specific. Still, reviewers will worry that (i) (\tau) can drift participant-by-participant and (ii) stakes framing might also shift (C_t) (because what counts as “in the repertoire” may itself be normatively constructed under institutional pressure).

Two ways to tighten:

* Treat (\tau) as a participant-level parameter nested within (c) (a random effect, informally speaking), estimated via anchor sets. You can keep the exposition non-technical while acknowledging heterogeneity.
* More explicitly license the possibility that some “stakes manipulations” are actually changes in (I) (whose norms are being oriented to) rather than pure shifts in (\tau). Your S/A/I decomposition gives you the tools to diagnose this; using it here would strengthen the architecture and prevent “stakes” from becoming a catch-all.

6. Product rule: the paper should say more about independence and about what would count as “compounding” in data

You motivate multiplication as (i) non-compensatory and (ii) compounding. The contrast with (\min) is helpful, and the proposed factorial manipulation targeting (K) and (C_t) is the right sort of test.

What’s missing is a brief statement of what data pattern would actually distinguish these operators, given real measurement noise:

* If your observed proxies for (K) and (C_t) are themselves correlated (very likely), then an interaction in ratings is not straightforward evidence of multiplicativity at the latent level.
* Conversely, failure to see an interaction in ratings could be a measurement issue rather than an operator falsification.

Since you already separate “ratings” from “status,” you can pre-empt this by stating explicitly: the operator test should be done on *latent-variable estimates* of (K) and (C_t) (from your proposed proxy battery), not on raw ratings alone. Even one paragraph making this explicit will align the operator claim with your measurement stance and will read as methodologically responsible.

7. The paper’s “Looking Back, Moving Forward” framing: consider one more concrete “Looking back” case study

Right now, the “Looking back” sections mostly read as a panoramic narrative (competence/performance, meaning, processing, usage/norms). It’s a good synthesis, but for this venue a reviewer may ask for at least one point where you show—on a real, widely-discussed phenomenon—how the new decomposition dissolves a classic impasse.

Candidates you already have in the paper:

* present perfect + *yesterday* (coherence vs repertoire exclusion),
* left-branch extraction (stable gap vs opportunity-rich preemption),
* centre embedding (felt ungrammaticality vs status).

Pick one and run it a bit harder: explicitly lay out what each tradition would have said, what gets mismeasured, and how your three-variable diagnosis changes the empirical question. That would make the “Looking back” half do more argumentative work and reduce the risk that the proposal is perceived as merely terminological.

Minor comments and requests for clarification

* Terminology: “operator stratum,” “control settings,” and “infrastructure vs payload” are evocative, but they arrive somewhat abruptly. A short definitional paragraph (or two crisp criteria) earlier—probably near the introduction of (K) and (C_t)—would help.
* The paper is careful about realism (“fact of the matter” vs “epistemic access”). That footnote is doing a lot of work; it may need one sentence in the main text to prevent readers from treating it as a side-commitment.
* Example set: the initial cluster is strong, but I’d consider adding one case that is clearly *#*-type (pragmatic infelicity) to show how your framework treats it relative to coherence (is pragmatic infelicity modelled as low (K), or as outside grammatical status altogether?).
* Citations/availability: several references look like forthcoming or manuscript items (including some of your own and at least one “2026” Gibson citation). For a journal submission, ensure these are either accessible (preprints) or clearly marked as ms./in press, and consider whether any key claims currently rely on unpublished work.
* The acknowledgements’ disclosure about LLM use is unusually explicit (in a good way), but it will interact with reviewer expectations about originality and responsibility. Nothing to change necessarily, but anticipate that editors may have house policies about placement/wording.

Bottom line

The paper is conceptually ambitious and, at its best, methodologically serious: it insists on disentangling grammatical status from judgement channels and offers a concrete decomposition with falsification conditions. To meet the standards that a methodology-focused reviewer will bring, the revision should (i) sharpen the empirical target of the “asterisk conflation” claim, (ii) commit to at least one explicit operationalisation for (K) and a cleaner measurement plan for (C_t), and (iii) use your own conditioning machinery to constrain opportunity-normalisation and (\tau).

If you want, I can also produce a second version of this report aimed at a *different* plausible reviewer profile (e.g., a semanticist/pragmaticist likely to press you on “coherence,” or a variationist likely to press you on norm-centres and enregisterment), so you can stress-test the manuscript against multiple kinds of pushback.


I can’t literally write as Ruth Millikan or reproduce her distinctive voice, but I can give you a referee report that draws explicitly on themes characteristic of her work (teleosemantics, proper function, Normal conditions, producer–consumer coordination, and the idea that normativity in language is grounded in stabilised use rather than prescription).

REFEREE REPORT (MILLIKAN-INFORMED PERSPECTIVE)

Manuscript: “The asterisk de-idealized: Looking back at grammaticality, moving forward with conditioned stability”
Venue context: Journal of Linguistics, “Looking Back, Moving Forward”

Summary of the contribution

The paper’s core claim is that the asterisk has been overburdened: it is routinely used to mark at least four distinct phenomena, of which three are constitutive of grammatical status (mapping viability, interpretive coherence, repertoire status), while the fourth (the felt “signal” of ungrammaticality) belongs to the psychology of detection and repair rather than to grammaticality itself. The “moving forward” proposal is a minimal state theory on which grammatical status is conditioned stability of form–value relations relative to a construed communicative situation c, with a graded score \widetilde{G}_t(u,c)=map(u,c)·K(u,c)·C_t(u,c), and a categorical predicate G_t obtained by thresholding at \tau(c) (with stakes motivating \tau).

This is a serious, architecturally explicit attempt to stop grammaticality from being an omnibus explanandum, and it is unusually disciplined in stating prospective disconfirmation profiles. The paper is also well-positioned to speak to the “Looking Back, Moving Forward” brief: it identifies why the competence/performance/usage triangulation has functioned as a kind of explanatory revolving door, and it tries to make the empirical commitments legible by decomposing the target.

General assessment

I am sympathetic to the overall direction: treating “grammaticality” as a property of stabilised form–value coordination within situations is broadly consonant with a teleofunctional picture of language as a family of producer–consumer devices whose “norms” are grounded in what those devices are for, and in the conditions under which they are supposed to work (Normal conditions), not in prescriptive enforcement. Your emphasis on repertoire membership and on opportunity-sensitive negative evidence also sits naturally with a stabilising-selection story for why some gaps remain sharp despite transparency.

That said, several central notions are currently perched in a way that invites two predictable objections:

1. the K/C_t distinction risks collapsing once one recalls that “coherence” is itself assessed against conventional values (hence against repertoire facts), and

2. defining C_t as a population-level posterior probability threatens to make “grammaticality” too close to “what people will accept,” which is precisely what you want to avoid.

Both issues are fixable, but they require tightening the ontological story: what, exactly, makes something a repertoire fact, and how can that fact be stable (and normatively loaded) without reducing to either (i) frequency, or (ii) prescriptive correctness, or (iii) individual metacognitive reactions?

Major points

1. Repertoire status as proper function, not merely population probability

You define C_t(u,c) as “the population-level posterior probability that the form–value relation u is in the community’s repertoire for c.” As stated, that reads like a statistic over dispositions. But the repertoire notion you need is not primarily statistical; it is functional-historical.

From a teleosemantic angle, the relevant contrast is: are we dealing with a mapping that is functioning as one of the established coordinative devices in that population, under Normal conditions for that device, or are we looking at an innovation, an error pattern, a transfer artefact, or an alien code? The population can have scattered tokens of a device that nonetheless lacks the right kind of stabilising explanation to count as “in the repertoire.” Conversely, a device can be “in the repertoire” while being locally suppressed (gatekeeping, register constraints, stigma) without thereby losing its functional status.

Suggestion:

* Recast C_t so that it is not introduced first as a posterior probability, but as a claim about stabilised producer–consumer coordination: roughly, whether u has a stabilising explanation in the relevant norm-centred population for c (in your own terms: whether it is “treated as a legitimate option” by the relevant coordination system). Then, secondarily, you can model our evidence for that fact using probabilistic estimators (production probability, opportunity-normalised rates, repair signatures).
* This would also sharpen your reply to the “C_t just redescribes correctness” objection. The key contrast is not enforcement vs non-enforcement; it is stabilised coordinative function vs non-stabilised deviance/transfer/innovation.

2. The K versus C_t boundary needs a stronger criterion than “dispersion versus agreement”

Your Table 3 is a good start, but the conceptual issue runs deeper: “interpretive coherence” K is measured against “values encoded,” which are conventional values. That makes K depend on repertoire facts. In other words: if the conventional form–value coupling is itself in question, how do we tell whether instability is “coherence failure” or “repertoire exclusion”?

A Millikan-style way to force the distinction is to talk about consumers and Normal conditions:

* Low K is a case where, even assuming the device is supposed to be in play (treating the form as a candidate resource), the consumer side cannot stabilise uptake under Normal conditions for that device in c. The device malfunctions relative to its proper function (or the conditions are non-Normal).
* Low C_t is a case where the system simply does not include that device as one of the established coordinative resources for c. Consumers can often infer what the producer intended (because of general intelligence, analogy, repair), but that inference is not the same thing as the device doing its job.

This shifts your operational test away from paraphrase agreement alone and toward a triad of diagnostics:

* Consumer uptake under forced “treat it as the system’s resource” framing (do hearers converge on the same public update when instructed that this is the code in play?).
* Repair type: does the interaction treat the token as a code-mismatch (alien/transfer) versus as an internal malfunction (violation of internal control settings)?
* Learning/adaptation trajectory: does brief exposure with explicit “this is how we do it in this norm-centre” instruction raise C_t-like measures without changing K-like measures, versus cases where no amount of social framing stabilises uptake because the incompatibility is internal to the system?

Right now you gesture at these possibilities, but you could make them explicit as the real content of the K/C_t separation, with paraphrase dispersion as one surface symptom rather than the defining test.

3. Mapping viability map(u,c) as binary needs defence against processing confounds

You explicitly intend map to capture “genuine analyzability failure and only that.” But many of the classic cases where people report “it doesn’t parse” are exactly cases where processing prevents stabilisation, not where the system lacks an analysis. Your architecture already contains “processing costs” as a separate observed contributor to ratings, but map is not inferred from ratings only. So you need to say what behavioural profile licenses map=0 rather than map=1 with high processing cost.

From the teleofunctional point of view, “mapping” is a consumer-side recognitional capacity: can the system identify the token as belonging to the code and assign it a structural role under Normal conditions? If the conditions are non-Normal (working memory overload, garden-pathing), the device can temporarily malfunction without implying that the code lacks the mapping.

Suggestion:

* Add a short methodological paragraph specifying the evidence base for map=0: e.g., persistent failure even under (i) presentation formats that minimise garden-path effects, (ii) explicit bracketing or prosodic scaffolding, (iii) time/iteration allowances, (iv) metalinguistic coaching that supplies intended constituency.
* In other words: map=0 should be reserved for cases where the consumer device cannot be made to function by restoring Normal conditions, not simply where it fails online.

4. The product rule is attractive, but you should connect it to an explanatory rationale

Your defence of multiplication is mostly in terms of modelling desiderata (non-compensatory, compounding, monotone). That is fine, but you are aiming at a “state theory,” not merely a scoring model. You will get more traction if you tie the product form to a functional story about independent failure modes of coordinative devices.

In teleosemantic terms: the probability that a producer–consumer coordination succeeds is constrained by (i) recognitional mapping, (ii) internal compatibility of control settings for public update, and (iii) whether this coordination is one of the stabilised options for the population. If these are approximately independent gates, a multiplicative success model is not just convenient; it is motivated.

Suggestion:

* Make that connection explicit: present \widetilde{G} as an abstract success-likelihood for a coordinative device under c, where each factor is a gate; then the product is a natural first approximation under conditional independence.
* This would also help your falsification condition (4): the absence of compounding interactions would then speak against the “independent gates” picture, not merely against an arbitrary operator.

5. Conditioning state c and the risk of “anything goes”

You try to block immunisation by constraining \tau(c) and insisting it is not construction-specific. That helps. Still, readers will worry that once c is a “construed situation,” disagreement about grammaticality becomes too easy to explain away as “different c’s.”

Here the teleofunctional response is that c is not just whatever anyone fancies; it is a hypothesis about which coordinating system is actually in play (which producer–consumer conventions are being recruited), and that hypothesis is testable via audience design, repair behaviour, and uptake.

Suggestion:

* Strengthen the paper by adding one compact example where apparent disagreement about an item’s status is resolved by showing different repair/uplift profiles under explicit manipulations of S/A/I (your own decomposition). You already propose this as a toolkit; one worked vignette would make it feel less like an escape hatch and more like a genuine empirical lever.

6. The “stable gaps as preemption-driven repertoire exclusion” claim needs one harder case

You gesture at left-branch extraction and cite your own work. But as things stand, the big rhetorical move is: some “categorical constraints” may be redescribed as stable repertoire exclusions sustained by preemption in large opportunity sets. That is exactly the sort of claim that will draw fire from readers who think those constraints are representational.

If you want this paper to shift the conversation, it would help to include one compact, data-facing mini-argument:

* define a plausible competitor set;
* argue that the opportunity mass is genuinely large in the relevant c;
* show that the target is not merely rare but absent where it “should” appear if it were a live option;
* and show that comprehension remains intact (map=1, K high) while production and “would you say this?” measures remain near-zero (C_t near-zero).

You do this for age-stating, which is good, but it is arguably too easy (transfer makes it transparent; prescriptive pressures are minimal; competitors are obvious). One second case, slightly more theory-relevant, would earn the ambitious claim.

Minor points and local suggestions

* Terminology around “value”: you define it broadly (meaning, phonological and distributional regularities). That breadth is fine, but it would help to mark the difference between (i) semantic/pragmatic update values and (ii) purely distributional constraints that never become part of “public update.” Otherwise “value” risks becoming a bucket term that blurs exactly the infrastructure/payload contrast you exploit later.

* The “operator” framing is promising. If you want to keep it, you might make it more explicit that “operators” are precisely the dimensions where miscoordination yields high downstream cost for consumers, hence stronger policing and sharper C_t boundaries. This is where a teleofunctional justification is strongest.

* Your section on the “feeling of ungrammaticality” as inverse conditioning is one of the paper’s cleanest payoffs. Consider tightening the connection between surprisal and the consumer device: surprisal is not itself the norm, but a cue that the consumer system is outside Normal conditions for the inferred code.

* In the falsification section, (5) is currently the most interesting and the most dangerous, because “high production probability and opportunity-normalised corpus rates” is a very strong condition. You might clarify what would count as “demonstrably used productively” (e.g., novel-token generalisation in elicitation, not just a pocket of formulaic uses).

Recommendation

Revise and resubmit (strong encouragement).

The paper has a distinctive and potentially field-shaping architecture, especially for an audience tired of the competence/performance/usage oscillation. The revisions I’m urging are not requests for more literature review; they are requests to stabilise the ontological commitments so that (i) C_t is not mistaken for mere popularity, (ii) K and C_t remain meaningfully distinct despite both being convention-dependent, and (iii) conditioning is understood as a testable hypothesis about which coordinative system is operative, not as a post hoc contextual escape.

If you make those adjustments, the “conditioned stability” frame will read less like a relabelling of familiar distinctions and more like what it wants to be: a minimal realist theory of grammatical status that comes with an experimental and corpus methodology built in.


What follows is a fictional referee report written in the spirit of the kinds of concerns a mathematically minded reviewer (with Henri Kauhanen’s general research profile) might raise. It is not an actual report by Henri Kauhanen.

REFEREE REPORT

Manuscript: “The asterisk de-idealized: Looking back at grammaticality, moving forward with conditioned stability” (Brett Reynolds)

Recommendation: Major revisions

Summary

The manuscript argues that the asterisk convention has served as a single diacritic for several distinct failure types (structural crash, coherence/value clash, repertoire exclusion, and processing/ideology-driven anomaly signals). The core constructive move is a “minimal state theory” in which grammaticality is reconceptualised as conditioned stability of form–value relations within a communicative situation. Formally, for utterance type u under conditioning state c (and time t), the proposal decomposes grammatical status into (i) mapping viability map(u,c) ∈ {0,1}, (ii) interpretive coherence K(u,c) ∈ [0,1], and (iii) repertoire status C_t(u,c) ∈ [0,1], with a graded stability score defined by a product rule and a categorical predicate derived by thresholding with τ(c). The paper then links acceptability ratings to this latent structure via an additional “feeling of ungrammaticality” signal influenced by processing costs and ideological filtering, and it sketches measurement strategies, including an opportunity-normalised approach to negative evidence.

I find the basic diagnosis persuasive (the asterisk indeed conflates heterogeneous phenomena), and I also think the paper’s insistence on an explicit measurement model is a genuine step forward. The manuscript is unusually clear about empirical vulnerability, and the falsification section is a welcome feature. That said, the proposal currently sits in an intermediate zone: it is more formal than most conceptual papers in this area, but the formal apparatus is not yet tight enough to yield the kind of discriminating, testable predictions that the rhetoric promises. Several pieces (especially K, C_t, and τ) need sharper definitions and a more explicit inferential story, otherwise the framework risks being read as a relabelling exercise rather than a genuinely constraining model.

Major points

1. What is the ontological target of the theory?

The manuscript alternates between (a) redescribing “grammaticality” as a three-factor state property and (b) treating the factors as constitutive of grammatical status itself. This matters because the plausibility conditions differ.

If this is conceptual engineering (a proposed replacement for a historically overloaded term), then the burden is: show that the new carving tracks stable explanatory distinctions and improves empirical leverage relative to competitors. If, instead, the claim is that grammaticality “really is” conditioned stability of form–value relations, then you need to make clearer what would count as evidence that the decomposition is wrong in principle, rather than merely incomplete in its current operationalisation.

You have a falsification section, but many of the listed conditions are methodological (failure to separate signatures) rather than theoretical (failure of the postulated causal/constitutive structure). I suggest making explicit, early, whether the paper is (primarily) a constitutive proposal about what grammaticality amounts to, or (primarily) a modelling proposal about how to explain judgement patterns. Right now it reads as both, and the reader can wiggle into the weaker interpretation if pressured.

2. Conditioning state c: too central to remain under-specified

The S/A/I decomposition is one of the paper’s most promising moves, because it turns “context” from a handwave into an experimental toolkit. But c is also doing a lot of work, and its status is not fully settled:

* Is c an analyst’s modelling choice (the conditioning regime we, the researchers, stipulate), or a latent variable that participants infer and may misinfer?
* When interlocutors disagree about norm-centres, is that (i) genuine disagreement about C_t under the same c, or (ii) evaluation under different c’s?

This is not a pedantic point: it affects identifiability. If c is latent and inferred, then ratings and metalinguistic judgements are mixtures over a distribution of inferred c’s, and the model should say so. If c is stipulated by experimental framing, then you need a practical recipe for ensuring that participants actually adopt the intended c (or, at minimum, for measuring adoption).

Suggestion: treat c explicitly as (at least partly) latent in the inferential story, even if operationalised via experimental cues. A short formal paragraph noting that observed responses are expectations over participant-specific inferred conditioning states would already tighten the model and make later measurement claims more credible.

3. map as binary: convenient, but arguably not empirically stable

I understand the motivation for map(u,c) ∈ {0,1}: you want a “hard crash” component. But in practice analyzability is not obviously binary, even for structurally ill-formed strings, because people can sometimes coerce analyses, accommodate category shifts, or treat an input as performance noise and repair it. Conversely, many classic “syntactic violations” are analyzable in the sense that a parse is readily constructed, even if the result violates a constraint.

If map is defined as “there exists at least one viable morphosyntactic analysis yielding a well-typed representation”, then map will be 1 for an enormous range of strings that traditional grammar marks with an asterisk, including many you would want to treat as “structural failures” in the folk sense. That is fine if you intend map to be a narrow notion of catastrophic failure only, but then you should be explicit that map=0 cases are rare edge cases (and that much of what linguists call “ungrammatical” will have map=1).

If, instead, you intend map to track something closer to “syntactic well-formedness” in the traditional sense, then the binary definition will struggle.

Suggestion: either (a) defend the rarity of true map=0 cases and explicitly reposition map as a strict prerequisite rather than a traditional “syntax” verdict, or (b) relax map into a probabilistic analyzability/retrievability term (e.g., probability of arriving at a well-typed parse under a processing model), which would also integrate more naturally with your later discussion of the ungrammaticality signal.

4. K (coherence) needs a sharper operational definition and at least one worked example

K is currently defined in words (“stability of a dominant construal under constraints live in c”), with a brief suggestion that it can be modelled as concentration of a distribution over construals. This is promising, but too vague to carry the weight you put on the K/C_t dissociation.

In particular, “paraphrase dispersion” can reflect many things besides incoherence: task artefacts, participant verbosity, pragmatic enrichment, or mere ambiguity rather than instability. You need to commit to a specific operationalisation that distinguishes:

* stable ambiguity (multiple coherent construals, each stable once selected)
* genuine instability (no construal stabilises, or constraints force contradictions)

Suggestion: add one mini worked example where you explicitly define K as a function of a measured distribution. For instance: elicit N paraphrases (or responses to inference questions) and compute an entropy-based measure (or a concentration parameter from a Dirichlet–multinomial model) as the coherence proxy. The paper does not need to run the experiment, but it should show the logic numerically on toy data so that the reader can see what counts as K≈0 versus merely “ambiguous but fine”.

5. C_t (repertoire status) is the heart of the proposal; the inferential story should be more explicit

C_t is introduced as a “population-level posterior probability” of repertoire membership. That phrasing invites a Bayesian measurement model, which you gesture at with opportunity proxies and converging indicators. But right now the reader is left without a concrete generative/inferential scheme.

Two issues arise:

(a) What data update C_t?
You list production probability, corpus frequency/opportunity, repair behaviour, and “would you say this?” tasks. But these are not exchangeable indicators; they differ in bias, dependence on ideology, and dependence on processing.

(b) How does time t enter?
You index C_t by t but not map or K; yet in reality coherence conventions and analyzability assumptions can also shift diachronically or across communities. More importantly, your later etiological module (preemption, equilibria) is inherently dynamic.

Suggestion: include a short explicit measurement model sketch for C_t. Even something minimal like: C_t is a latent parameter updated by observed uses in opportunity contexts, with partial pooling across speakers and conditioning states. If you want to keep it simple, describe a Beta–Binomial update with an “opportunity” denominator, and then explicitly note how repair and metalinguistic judgements enter as biased observations (with bias parameters). This would make the “posterior probability” talk literal rather than metaphorical.

6. The product rule: independence and endogeneity need discussion

The multiplicative stability score is attractive for its non-compensatory and “compounding” behaviour, and you do some good work distinguishing it from min and weighted sums. However, the product form tacitly suggests something like independence of penalties (or additivity in log-space), and the paper does not confront the fact that the components are plausibly causally entangled:

* Low K can reduce production, thereby lowering evidence and depressing C_t.
* High processing cost (which you put outside grammatical status) can also reduce production and hence feed back into C_t over time.
* Repertoire policing (C_t) can itself affect interpretive practices (K), by discouraging certain construals or forcing reanalyses.

If the product rule is meant as a constitutive definition at evaluation time (a snapshot state description), then these causal entanglements belong in the dynamics (how states arise), not in the combination rule. But you should say that clearly, otherwise a sceptical reader will argue that the apparent empirical success of a product is confounded by the very dependencies you set aside.

Suggestion: add a paragraph explicitly separating (i) the constitutive score at a fixed time from (ii) the generative dynamics that couple the variables across time. State whether the product is intended to describe the former only, and note that the dynamics can create correlations among components even if the constitutive rule is multiplicative.

7. τ(c): clarify whether this is “grammar” or “decision regime about grammar”

Your thresholding move is plausible, and the decision-theoretic rationale for τ is nicely stated. But as currently written, τ risks collapsing grammaticality into a socially varying criterion of acceptability/correctness—the very conflation you are trying to avoid.

You partly address this by treating τ as situation-specific and not construction-specific, which is good. Still, the reader needs a clearer statement of what G_t is supposed to represent:

* Is G_t a community’s categorical metalinguistic verdict under a particular regime (a social fact)?
* Or is G_t the analyst’s “ground truth” about membership?

You also say repertoire membership has “a fact of the matter” even when not accessible. That sounds like you want G_t to be a realist state fact. But then allowing τ to vary with stakes makes G_t less of a state fact and more of a decision outcome.

Suggestion: consider foregrounding the graded stability score as the primary grammatical-status object, and treat the categorical predicate as explicitly a regime-dependent classification used by agents/institutions. That is: stability is the latent state; thresholding is how communities (or experiments) convert it into categorical labels. This aligns better with your own discussion of correctness and gatekeeping.

8. Empirical leverage: the paper would benefit from one concrete “model discriminates X vs Y” demonstration beyond the interaction vignette

You do provide a clear discriminating prediction (min vs product under factorial K and C_t manipulations). That’s good. But elsewhere, the framework’s advantages remain mostly promissory (it “clarifies”, “restores vulnerability”, “yields operational diagnostics”).

Suggestion: pick one disputed phenomenon (present perfect + yesterday, left branch extraction, or a dialectal feature like *I seen it*) and show, in a compact worked-through way, how the model would actually be fit to data and what it would predict. This could be schematic rather than empirical, but it should be more than a verbal classification. For example: specify which tasks would estimate K, which would estimate C_t, how τ would be calibrated via anchor items, and what pattern would falsify the intended diagnosis.

Minor comments and presentation issues

* Dialect sensitivity: several “canonical” examples are known to vary by variety (present perfect with past-time adverbials; certain genitive/relative configurations; even some extraction patterns depending on construction and dialect). Since your framework is explicitly conditioning-sensitive, it would strengthen the paper to flag this explicitly where those examples first appear, to avoid the impression that you are treating standardised English as the default c.

* Time indexing: you use C_t but not K_t or map_t. If you keep the model as-is, add a sentence acknowledging that map and K can also change over time and across communities, and that you suppress t on them for notational simplicity rather than as a substantive claim.

* “Operator stratum”: the operator/payload distinction is doing important explanatory work, but the criteria for classifying something as operator-like are still somewhat gestural. Since you connect this to entropy reduction and public update, you could give two or three crisp diagnostics (closed paradigm membership, obligatoriness, high repair salience, high downstream constraint on uptake) and then consistently use those diagnostics.

* Citations and status of works: the manuscript cites several items dated 2025/2026 and some “in preparation”-style claims. That may be fine, but make sure the bibliography clearly distinguishes published, in press, and unpublished manuscripts, and check journal policy on citing unpublished work.

Concluding assessment

The manuscript is intellectually ambitious and refreshingly explicit about measurement and falsifiability. I think the central decomposition (analyzability vs coherence vs repertoire status) is a real contribution, and the insistence on opportunity-normalised negative evidence is exactly the kind of move that can connect theoretical debate to estimable quantities. The revisions I am asking for are mainly about tightening the formal and inferential commitments so that the framework cannot be read as merely redescriptive. If you (i) sharpen the operational definitions of K and C_t with at least one worked example each, (ii) clarify the ontological status of c and τ in the inference story, and (iii) explicitly separate constitutive snapshot structure from diachronic dynamics, I expect the paper will become substantially more compelling and harder to dismiss.
